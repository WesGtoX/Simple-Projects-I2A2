{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SqueezeNets - Deep Learning na categoria Peso-Pena.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A maioria das pesquisas/competições de Deep Learning focam somente na acurácia, mas esse pensamento é insustentável quando confrontado com uma situação de deploy da solução, principalmente quando pensamos em escalabilidade ou portabilidade.\n",
    "Problemas comums no sistema de produção, o tamanho dos modelos e a latência de inferência são os grandes culpados no deploy de maioria das ConvNets populares (VGG, ResNet, Inception, etc.) ocupando facilmente várias centenas de MB com seus parâmetros e apresentando latência considerável na inferência. \n",
    "\n",
    "# Pesos-Pesados para comparação\n",
    "<img src=\"Imagens/CNNs.png\">\n",
    "\n",
    "<h2 style='padding: 10px'>VGG16 x AlexNet</h2><table class='table table-striped'> <thead> <tr> <th>VGG16</th> <th>AlexNet</th></tr> </thead> <tbody> <tr> <td>138M Parâmetros ~ 528MB</td> <td>60M Parâmetros ~ 285MB</td>  </tr> <tr> <td>16 Camadas </td> <td>8 Camadas</td> </tr> <tr> <td>Filtros 3x3</td> <td>Filtros 5x5 e 3x3</td> </tr> <tr> <td>top-5 accuracy de 92.3 % na ImageNet.</td> <td>top-5 accuracy de 80.6% na ImageNet.</td>  </tr> </tbody> </table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mas não se aborreça, pequeno padawan. As Redes Peso-Pena como SqueezeNets e MobileNets vieram para sanar este problema !\n",
    "\n",
    "<table><tr><td><img src=\"Imagens/motivacao.png\"></td><td><img src=\"Imagens/motivacao2.png\"></td></tr></table>\n",
    "\n",
    "\n",
    "\n",
    "# Qual a Mágica ?\n",
    "\n",
    "Na verdade, é um fato conhecido que as ConvNets tem muito mais conexões do que precisam para fazer seu trabalho. \n",
    "No paper “Deep Compression” de Han et al. o autor prova que o tamanho da VGG - 16 pode ser reduzido por um fator 49 somente pela poda de conexões sem importância, mantendo a acurácia da rede. \n",
    "\n",
    "# SqueezeNets vs \"ConvNets\"\n",
    "\n",
    "Principais pontos de diferença com uma ConvNet:\n",
    "- Maioria dos kernels reduzidos de 3x3 para 1x1 (Redução de 9x nos parâmetros)\n",
    "- Canais de entrada nos filtros 3x3 restantes são alterados para diminuir parâmetros \n",
    "  -  i.e.  NParam = (input channels) * (number of filters) * (3*3).\n",
    "- Ausência de camadas FC \n",
    "- Output gerada com o uso de Avg. Pooling\n",
    "- Introdução do módulo <font color=red>__FIRE__</font> **( Squeeze + Expand )**\n",
    "\n",
    "# O quê ? Filtro 1x1 ?\n",
    "\n",
    "\n",
    "Embora a convolução de 1 x 1 seja uma técnica de \"agrupamento de features\", há mais do que apenas a soma de \"features\" em vários canais/\"mapas de features\" de uma determinada camada. A convolução de 1x1 atua como uma transformação dependente de coordenadas no espaço de filtro.\n",
    "\n",
    "É importante notar aqui que esta transformação é estritamente linear, mas na maior parte da aplicação da convolução 1x1, ela é sucedida por uma camada de ativação não linear como ReLU. Essa transformação é aprendida através da descida gradiente (estocástica). Mas uma distinção importante é que ela sofre com menos adaptação devido ao tamanho menor do kernel (1x1).\n",
    "\n",
    "A convolução 1x1 foi criada para gerar uma rede mais profunda sem simplesmente empilhar mais camadas. Substituindo alguns filtros por uma camada menor com uma mistura de convoluções 1x1 e 3x3. De certa forma, essa abordagem pode ser vista como \"net widening\" em vez do clássico \"net deepening\". \n",
    "\n",
    "Na arquitetura GoogLeNet, por exemplo, a convolução de 1x1 é usada para três propósitos:\n",
    "\n",
    "- Para tornar a rede mais profunda, adicionando um “módulo de widening”\n",
    "- Para reduzir as dimensões dentro deste \"módulo de widening\".\n",
    "- Para adicionar mais não-linearidade por ter ReLU imediatamente após cada convolução de 1x1.\n",
    "\n",
    "<img src=\"Imagens/conv1x1.png\">\n",
    "\n",
    "# O módulo __FIRE__\n",
    "\n",
    "<img src=\"Imagens/fogo.gif\">\n",
    "\n",
    "\n",
    "Um módulo <font color=red>__FIRE__</font>  é composto por: \n",
    "- Uma camada de convolução de compressão (que tem apenas filtros 1x1), alimentando em uma camada de expansão que tem uma mistura de convolução 1x1 e 3x3 filtros;\n",
    "- Em um módulo Fire, **s1x1** é o número de filtros na camada de \"squeeze\" (totalmente 1x1), **e1x1** é o número de filtros 1x1 na camada de expansão e **e3x3** é o número de filtros 3x3 na camada de expansão. \n",
    "- Normalmente definimos s1x1 como menor que (e1x1 + e3x3), de modo que a camada \"squeeze\" ajuda a limitar o número de canais de entrada para os filtros 3x3, reduzindo os parâmetros.\n",
    "\n",
    "<img src=\"Imagens/fire.png\">\n",
    "\n",
    "# A Arquitetura Completa \n",
    "\n",
    "\n",
    "<img src=\"Imagens/squeezefull.png\">\n",
    "\n",
    "***\n",
    "\n",
    "**Tá bom, chega de teoria, hora de botar a mão na massa !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos importando todos os pacotes que iremos usar :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os, argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois criamos a pasta para guardar o modelo treinado e o dataset do MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!mkdir MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora criamos as variáveis *\"placeholders\"* do tensorflow para guardar os dados de entrada/saída e a taxa de passagem da camada dropout. Além disso, criamos as variáveis de controle para guardar o número de classes e o limite do gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,(None,28*28),name='X')\n",
    "y = tf.placeholder(tf.float32,(None,10),name='Y')\n",
    "keep_prob = tf.placeholder(tf.float32,name='dropout')\n",
    "classCount = 10\n",
    "grad_limit = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, criamos uma estrutura para guardar a arquitetura, inserimos nesta a entrada **x** e definimos a quantidade de filtros convolucionais e \"squeezes\" da camada FIRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "x_image = tf.reshape(x,(-1,28,28,1))\n",
    "activations.append(x_image)\n",
    "filters = [16,16,32,32,48,48,64,64]\n",
    "squeezes= [4,4,8, 8, 12, 12, 16, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora o **core** do nosso código, vamos definir as funções que realizam a compressão e expansão dos dados, além de uma funçãozinha para percorrer o grafo da arquitetura e contar os parâmetros. (*getSize()*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(inputs,squeezeTo):\n",
    "    with tf.name_scope('squeeze'):\n",
    "        inputSize = inputs.get_shape().as_list()[3]\n",
    "        w = tf.Variable(tf.truncated_normal([1,1,inputSize,squeezeTo]))\n",
    "        h = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))        \n",
    "    return h\n",
    "\n",
    "def expand(inputs,expandTo):\n",
    "    with tf.name_scope('expand'):\n",
    "        squeezeTo = inputs.get_shape().as_list()[3]\n",
    "        w = tf.Variable(tf.truncated_normal([1,1,squeezeTo,expandTo]))\n",
    "        h1x1 = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,squeezeTo,expandTo]))\n",
    "        h3x3 = tf.nn.relu(tf.nn.conv2d(inputs,w,[1,1,1,1],'SAME'))\n",
    "        h = tf.concat([h1x1,h3x3],3)\n",
    "    return h\n",
    "def fire(inputs,squeezeTo,expandTo):\n",
    "    h = squeeze(inputs,squeezeTo)\n",
    "    h = expand(h,expandTo)\n",
    "    h = tf.clip_by_norm(h,grad_limit)\n",
    "    activations.append(h)\n",
    "\n",
    "\n",
    "def getSize():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value \n",
    "        total_parameters += variable_parametes\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos agora a arquitetura, combinando o módulo FIRE com camadas convolucionais e maxpooling para diminuir os parâmetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('conv1'):\n",
    "    w = tf.Variable(tf.truncated_normal([3,3,1,64]))\n",
    "    h = tf.nn.relu(tf.nn.conv2d(activations[-1],w,[1,2,2,1],'SAME'))\n",
    "    activations.append(h)\n",
    "\n",
    "with tf.name_scope('maxpool1'):\n",
    "    h = tf.nn.max_pool(activations[-1],[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    activations.append(h)\n",
    "\n",
    "for i in range(0,2):\n",
    "    with tf.name_scope('fire'+str(i+2)):\n",
    "        fire(activations[-1],squeezes[i],filters[i])\n",
    "\n",
    "with tf.name_scope('maxpool2'):\n",
    "    h = tf.nn.max_pool(activations[-1],[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    activations.append(h)\n",
    "\n",
    "for i in range(2,4):\n",
    "    with tf.name_scope('fire'+str(i+2)):\n",
    "        fire(activations[-1],squeezes[i],filters[i])\n",
    "\n",
    "with tf.name_scope('maxpool3'):\n",
    "    h = tf.nn.max_pool(activations[-1],[1,3,3,1],[1,2,2,1],'SAME')\n",
    "    activations.append(h)\n",
    "\n",
    "for i in range(4,7):\n",
    "    with tf.name_scope('fire'+str(i+2)):\n",
    "        fire(activations[-1],squeezes[i],filters[i])\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    h = tf.nn.dropout(activations[-1],keep_prob)\n",
    "    activations.append(h)\n",
    "\n",
    "with tf.name_scope('conv10'):\n",
    "    input_shape = activations[-1].get_shape().as_list()[3]\n",
    "    w = tf.Variable(tf.truncated_normal([1,1,input_shape,classCount]))\n",
    "    h = tf.nn.relu(tf.nn.conv2d(activations[-1],w,[1,1,1,1],'SAME'))\n",
    "    activations.append(h)\n",
    "\n",
    "with tf.name_scope('avgpool'):\n",
    "    input_shape = activations[-1].get_shape().as_list()[2]\n",
    "    h = tf.nn.avg_pool(activations[-1],[1,input_shape,input_shape,1],[1,1,1,1],'VALID')\n",
    "    h = tf.squeeze(h,[1,2])\n",
    "    activations.append(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a arquitetura decidida, definimos a saída da rede usando um Softmax e, por fim, a nossa função de perda e otimizador.\n",
    "\n",
    "As variáveis *correct_prediction* e *accuracy* são variáveis de controle que podem ser acessadas pelo tensorboard ou requisitadas pela sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-32b6689136f3>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_conv = tf.nn.softmax(activations[-1],name='Output')\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=activations[-1], labels=y)\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='Accuracy')\n",
    "\n",
    "tf.summary.scalar('accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar agora, salvando sempre quando atingimos a melhor acurácia de validação !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-c1ed2db4c1f9>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:219: retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/wesley/Projects-I2A2/deep_learning/venv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Started\n",
      "Números de parâmetros : 34624 \n",
      "Epoch 0, training accuracy  5.33%, validation accuracy 8.29%\n",
      "Epoch 50, training accuracy  22.00%, validation accuracy 20.44%\n",
      "Epoch 100, training accuracy  34.00%, validation accuracy 37.54%\n",
      "Epoch 150, training accuracy  58.00%, validation accuracy 63.06%\n",
      "Epoch 200, training accuracy  80.67%, validation accuracy 79.25%\n",
      "Epoch 250, training accuracy  88.00%, validation accuracy 87.18%\n",
      "Epoch 300, training accuracy  90.67%, validation accuracy 88.85%\n",
      "Epoch 350, training accuracy  90.00%, validation accuracy 89.90%\n",
      "Epoch 400, training accuracy  90.67%, validation accuracy 92.52%\n",
      "Epoch 450, training accuracy  90.67%, validation accuracy 91.02%\n",
      "Epoch 500, training accuracy  90.00%, validation accuracy 91.61%\n",
      "Epoch 550, training accuracy  92.00%, validation accuracy 92.68%\n",
      "Epoch 600, training accuracy  95.33%, validation accuracy 93.65%\n",
      "Epoch 650, training accuracy  93.33%, validation accuracy 94.12%\n",
      "Epoch 700, training accuracy  90.00%, validation accuracy 93.04%\n",
      "Epoch 750, training accuracy  92.67%, validation accuracy 95.21%\n",
      "Epoch 800, training accuracy  93.33%, validation accuracy 95.31%\n",
      "Epoch 850, training accuracy  94.67%, validation accuracy 93.01%\n",
      "Epoch 900, training accuracy  94.00%, validation accuracy 95.42%\n",
      "Epoch 950, training accuracy  92.67%, validation accuracy 94.52%\n",
      "Epoch 1000, training accuracy  91.33%, validation accuracy 94.06%\n",
      "Training Finished ! Best Accuracy : 0.954200\n",
      "Model saved in file: models/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "summaryWriter = tf.summary.FileWriter('./Tensorboard',sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "mnist = input_data.read_data_sets('MNIST', one_hot=True)\n",
    "saver = tf.train.Saver()\n",
    "start_time = time.time()\n",
    "print('Started')\n",
    "print('Números de parâmetros : %d '%getSize())\n",
    "run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "run_metadata = tf.RunMetadata()\n",
    "best_acc = 0\n",
    "for i in range(1001):\n",
    "    batch = mnist.train.next_batch(150)\n",
    "    if i%50 == 0:\n",
    "        summary, train_accuracy = sess.run([merged, accuracy],\n",
    "                                    feed_dict={x: batch[0], y: batch[1],keep_prob: 1},\n",
    "                                    options=run_options,\n",
    "                                    run_metadata=run_metadata)\n",
    "        summaryWriter.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "        summaryWriter.add_summary(summary, i)\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels,keep_prob: 1})\n",
    "        \n",
    "        print(\"Epoch %d, training accuracy  %2.2f%%, validation accuracy %2.2f%%\"%(i, 100*train_accuracy,\n",
    "                                                100*valid_accuracy))\n",
    "        if(best_acc < valid_accuracy):\n",
    "            best_acc = valid_accuracy\n",
    "            save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "\n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1],keep_prob:.5})\n",
    "\n",
    "print(\"Training Finished ! Best Accuracy : %f\"%best_acc)\n",
    "save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que treinamos nossa squeezenet e salvamos seu modelo, como usar isso denovo ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência usando um modelo pré-treinado.\n",
    "\n",
    "Para usar o modelo, primeiro vamos executar uma função originalmente nativa do tensorflow, mas modificada para a nossa squeeze, que \"poda\" o grafo do modelo de seus nós de treinamento, congelando sua forma e gerando um modelo final de tamanho reduzido!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_folder,output_node):\n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    # We precise the file fullname of our freezed graph\n",
    "    absolute_model_folder = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph ='frozen_model.pb'\n",
    "\n",
    "    # Before exporting our graph, we need to precise what is our output node\n",
    "    # This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "    # NOTE: this variable is plural, because you can have multiple output nodes\n",
    "    output_node_names = output_node\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We import the meta graph and retrieve a Saver\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "    # We retrieve the protobuf graph definition\n",
    "    graph = tf.get_default_graph()\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "\n",
    "    # We start a session and restore the graph weights\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(\n",
    "            sess,  # The session is used to retrieve the weights\n",
    "            input_graph_def,  # The graph_def is used to retrieve the nodes\n",
    "            output_node_names.split(\",\")  # The output node names are used to select the usefull nodes\n",
    "        )\n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "INFO:tensorflow:Froze 23 variables.\n",
      "Converted 23 variables to const ops.\n",
      "206 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph(\"models\",\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, agora você tem um modelo treinado e congelado da Squeeze para o MNIST !\n",
    "\n",
    "Vamos testar essa belezinha, começamos com uma função para importar o grafo do arquivo congelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    \"\"\"Load a (frozen) Tensorflow model into memory.\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(frozen_graph_filename, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos o grafo congelado e recuperamos os tensores pelos nomes que definimos na primeira etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST', one_hot=True)\n",
    "graph = load_graph('frozen_model.pb')\n",
    "input = graph.get_tensor_by_name('X:0')\n",
    "input_labels = graph.get_tensor_by_name('Y:0')\n",
    "output = graph.get_tensor_by_name('Output:0')\n",
    "dropout = graph.get_tensor_by_name('dropout:0')\n",
    "accuracy = graph.get_tensor_by_name('Accuracy:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hora de fazer uma inferência ! Repare no tempo e confira a acurácia com sua acurácia de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira inferência : Time elapsed  : 0.172629 s\n",
      "Segunda Inferência : Time elapsed  : 0.001190 s\n",
      "Network Output : \n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Label from Dataset : \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Test set accuracy : \n",
      "0.9433\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "start = time.time()\n",
    "res = sess.run(output, feed_dict={input: mnist.test.images[5].reshape(1,784),dropout: 1})\n",
    "end = time.time()\n",
    "print(\"Primeira inferência : Time elapsed  : %f s\"%(end-start))\n",
    "start = time.time()\n",
    "res = sess.run(output, feed_dict={input: mnist.test.images[5].reshape(1,784),dropout: 1})\n",
    "end = time.time()\n",
    "print(\"Segunda Inferência : Time elapsed  : %f s\"%(end-start))\n",
    "print(\"Network Output : \")\n",
    "print(res)\n",
    "print(\"Label from Dataset : \")\n",
    "print(mnist.test.labels[5])\n",
    "print(\"Test set accuracy : \")\n",
    "print(sess.run(accuracy, feed_dict={input: mnist.test.images, input_labels: mnist.test.labels,dropout: 1}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos conferir visualmente a imagem processada :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pixels = mnist.test.images[5]*255\n",
    "pixels = np.array(pixels, dtype='uint8')\n",
    "\n",
    "pixels = pixels.reshape((28, 28))\n",
    "\n",
    "\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Referência :\n",
    "[SqueezeNet Paper](https://arxiv.org/abs/1602.07360)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
