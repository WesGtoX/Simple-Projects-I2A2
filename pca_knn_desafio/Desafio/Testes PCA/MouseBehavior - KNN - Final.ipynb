{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv files and creating dataframes\n",
    "df_evandro = pd.read_csv('Evandro.csv', sep=';', encoding='latin-1')\n",
    "df_celso = pd.read_csv('Celso.csv', sep=';', encoding='latin-1')\n",
    "df_eliezer = pd.read_csv('Eliezer.csv', sep=';', encoding='latin-1')\n",
    "df_rafael = pd.read_csv('Rafael.csv', sep=',', encoding='latin-1')\n",
    "df_thiago = pd.read_csv('Thiago.csv', sep=';', encoding='latin-1')\n",
    "\n",
    "# drop NaN values (if any)\n",
    "df_evandro.dropna(inplace=True)\n",
    "df_celso.dropna(inplace=True)\n",
    "df_eliezer.dropna(inplace=True)\n",
    "df_rafael.dropna(inplace=True)\n",
    "df_thiago.dropna(inplace=True)\n",
    "\n",
    "# drop useless data\n",
    "df_evandro.drop(['Date', 'Time', 'Event Type'], axis=1, inplace=True)\n",
    "df_celso.drop(['Date', 'Time', 'Event Type'], axis=1, inplace=True)\n",
    "df_eliezer.drop(['Date', 'Time', 'Event Type'], axis=1, inplace=True)\n",
    "df_rafael.drop(['Date', 'Time', 'Event Type'], axis=1, inplace=True)\n",
    "df_thiago.drop(['Date', 'Time', 'Event Type'], axis=1, inplace=True)\n",
    "\n",
    "# getting rid of outliers by calculating the Z-score across all columns and deleting \n",
    "# rows whose any of the values is below the threshold\n",
    "df_evandro = df_evandro[(np.abs(stats.zscore(df_evandro)) < 2).all(axis=1)].reset_index(drop=True)\n",
    "df_celso = df_celso[(np.abs(stats.zscore(df_celso)) < 2).all(axis=1)].reset_index(drop=True)\n",
    "df_eliezer = df_eliezer[(np.abs(stats.zscore(df_eliezer)) < 2).all(axis=1)].reset_index(drop=True)\n",
    "df_rafael = df_rafael[(np.abs(stats.zscore(df_rafael)) < 2).all(axis=1)].reset_index(drop=True)\n",
    "df_thiago = df_thiago[(np.abs(stats.zscore(df_thiago)) < 2).all(axis=1)].reset_index(drop=True)\n",
    "\n",
    "# DAQUI EM DIANTE, DEIXAR APENAS OS DOIS QUE ESTÃO SENDO TESTADOS\n",
    "\n",
    "# set the maximum row numbers\n",
    "#maxRows = [df_evandro.shape[0], df_celso.shape[0]]\n",
    "#maxRows.sort()\n",
    "\n",
    "maxRows = [df_eliezer.shape[0], df_thiago.shape[0]]\n",
    "maxRows.sort()\n",
    "\n",
    "# slice dataframes in order to equalize the length\n",
    "#df_evandro = df_evandro.loc[:maxRows[0]-1,:]\n",
    "#df_celso = df_celso.loc[:maxRows[0]-1,:]\n",
    "df_eliezer = df_eliezer.loc[:maxRows[0]-1,:]\n",
    "#df_rafael = df_rafael.loc[:maxRows[0]-1,:]\n",
    "df_thiago = df_thiago.loc[:maxRows[0]-1,:]\n",
    "\n",
    "#print(df_evandro.shape[0], df_celso.shape[0])\n",
    "print(df_eliezer.shape[0], df_thiago.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for creating new variables and standardizing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatures(df):\n",
    "    offset_list, xm_list, ym_list, xstd_list, ystd_list, distm_list, diststd_list, arct_list = ([] for i in range(8))\n",
    "\n",
    "    # deleting rows with coordinate X being 0\n",
    "    df = df[df['Coordinate X'] != 0]\n",
    "\n",
    "    # filtering unique id == 1 \n",
    "    ulist = df['EventId'].unique()\n",
    "    for u in ulist:\n",
    "        df_unique = df[df['EventId'] == u]\n",
    "        if df_unique.shape[0] == 1: # original is \"== 1\"\n",
    "            df = df[df['EventId'] != u]\n",
    "\n",
    "    # list of unique id with occurrence > 1\n",
    "    ulist = df['EventId'].unique()\n",
    "\n",
    "    for u in ulist:\n",
    "        df_unique = df[df['EventId'] == u]\n",
    "\n",
    "        # adding mean\n",
    "        x_mean = df_unique['Coordinate X'].mean()\n",
    "        y_mean = df_unique['Coordinate Y'].mean()\n",
    "        xm_list.append(x_mean)\n",
    "        ym_list.append(y_mean)\n",
    "        \n",
    "        # adding std\n",
    "        xstd_list.append(df_unique['Coordinate X'].std())\n",
    "        ystd_list.append(df_unique['Coordinate Y'].std())\n",
    "\n",
    "        # calculating euclidean distances\n",
    "        arr = np.array([(x, y) for x, y in zip(df_unique['Coordinate X'], df_unique['Coordinate Y'])])\n",
    "        dist = [np.linalg.norm(arr[i+1]-arr[i]) for i in range(arr.shape[0]-1)]\n",
    "        ideal_dist = np.linalg.norm(arr[arr.shape[0]-1]-arr[0])\n",
    "\n",
    "        # adding offset\n",
    "        offset_list.append(sum(dist)-ideal_dist)\n",
    "\n",
    "        # adding distance mean\n",
    "        distm_list.append(np.asarray(dist).mean())\n",
    "\n",
    "        # adding distance std deviation\n",
    "        diststd_list.append(np.asarray(dist).std())\n",
    "\n",
    "    # create df subset with the new features\n",
    "    df_subset = pd.DataFrame(ulist, columns=['EventId'])\n",
    "    \n",
    "    df_subset['Dist Mean'] = distm_list\n",
    "    df_subset['Dist Std Dev'] = diststd_list\n",
    "    df_subset['Offset'] = offset_list\n",
    "\n",
    "    # drop EventId\n",
    "    df_subset.drop(['EventId'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_subset\n",
    "\n",
    "def standardize(df):\n",
    "    # instanciate StandardScaler object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # compute the mean and std to be used for later scaling\n",
    "    scaler.fit(df)\n",
    "\n",
    "    # perform standardization by centering and scaling\n",
    "    scaled_features = scaler.transform(df)\n",
    "\n",
    "    return pd.DataFrame(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new features from existing variables\n",
    "#df_evandro = createFeatures(df_evandro)\n",
    "#df_celso = createFeatures(df_celso)\n",
    "df_eliezer = createFeatures(df_eliezer)\n",
    "#df_rafael = createFeatures(df_rafael)\n",
    "df_thiago = createFeatures(df_thiago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Shuffling and splitting into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the maximum row numbers\n",
    "maxRows = [df_eliezer.shape[0], df_thiago.shape[0]]\n",
    "#(ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#df_evandro.shape[0], df_celso.shape[0], #df_eliezer.shape[0], #df_rafael.shape[0], #df_thiago.shape[0]\n",
    "maxRows.sort()\n",
    "\n",
    "# slice dataframes in order to equalize the length\n",
    "#df_evandro = df_evandro.loc[:maxRows[0]-1,:]\n",
    "#df_celso = df_celso.loc[:maxRows[0]-1,:]\n",
    "df_eliezer = df_eliezer.loc[:maxRows[0]-1,:]\n",
    "#df_rafael = df_rafael.loc[:maxRows[0]-1,:]\n",
    "df_thiago = df_thiago.loc[:maxRows[0]-1,:]\n",
    "\n",
    "print(df_eliezer.shape[0], df_thiago.shape[0])\n",
    "#(ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#df_evandro.shape[0], df_celso.shape[0], df_eliezer.shape[0], #df_rafael.shape[0], #df_thiago.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RODAR VÁRIAS VEZES A PARTIR DAQUI, CADA VEZ O DATASET VAI SER MISTURADO E A ACURÁCIA PODE SER DIFERENTE\n",
    "\n",
    "#df_evandro_shuffle = df_evandro.sample(frac=1).reset_index(drop=True)\n",
    "#df_celso_shuffle = df_celso.sample(frac=1).reset_index(drop=True)\n",
    "df_eliezer_shuffle = df_eliezer.sample(frac=1).reset_index(drop=True)\n",
    "#df_rafael_shuffle = df_rafael.sample(frac=1).reset_index(drop=True)\n",
    "df_thiago_shuffle = df_thiago.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# PESSOA QUE QUER VERIFICAR (70% DE DADOS PRA TREINO E 30% PARA TESTE)\n",
    "#df_evandro_train = df_evandro_shuffle.loc[:(df_evandro_shuffle.shape[0]-1)*0.7]\n",
    "#df_evandro_test = df_evandro_shuffle.loc[(df_evandro_shuffle.shape[0]*0.7):]\n",
    "\n",
    "df_eliezer_train = df_eliezer_shuffle.loc[:(df_eliezer_shuffle.shape[0]-1)*0.7]\n",
    "df_eliezer_test = df_eliezer_shuffle.loc[(df_eliezer_shuffle.shape[0]*0.7):]\n",
    "\n",
    "# OUTRA PESSOA (NÃO PRECISA DO DATASET DE TESTE, PEGA APENAS 70% PARA TREINO)\n",
    "#df_celso_train = df_celso_shuffle.loc[:(df_celso_shuffle.shape[0]-1)*0.7]\n",
    "\n",
    "df_thiago_train = df_thiago_shuffle.loc[:(df_thiago_shuffle.shape[0]-1)*0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing training datasets\n",
    "\n",
    "# PADRONIZAR TREINO E TESTE DA PESSOA QUE QUER VERIFICAR (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#df_evandro_train = standardize(df_evandro_train)\n",
    "#df_evandro_test = standardize(df_evandro_test)\n",
    "\n",
    "df_eliezer_train = standardize(df_eliezer_train)\n",
    "df_eliezer_test = standardize(df_eliezer_test)\n",
    "\n",
    "# PADRONIZAR TREINO DA OUTRA PESSOA (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#df_celso_train = standardize(df_celso_train)\n",
    "\n",
    "df_thiago_train = standardize(df_thiago_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running PCA on training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying PCA and concat on train datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# PCA NO DATASET DE TREINO DA PESSOA QUE QUER VERIFICAR (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#principalComponents = pca.fit_transform(df_evandro_train)\n",
    "#df_evandro_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "#df_evandro_train['Label'] = ['Evandro' for s in range(df_evandro_train.shape[0])]\n",
    "\n",
    "principalComponents = pca.fit_transform(df_eliezer_train)\n",
    "df_eliezer_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "df_eliezer_train['Label'] = ['Eliezer' for s in range(df_eliezer_train.shape[0])]\n",
    "\n",
    "# PCA NO DATASET DE TESTE DA PESSOA QUE QUER VERIFICAR (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "\n",
    "#principalComponents = pca.fit_transform(df_evandro_test)\n",
    "#df_evandro_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "#df_evandro_test['Label'] = ['Evandro' for s in range(df_evandro_test.shape[0])]\n",
    "\n",
    "principalComponents = pca.fit_transform(df_eliezer_test)\n",
    "df_eliezer_test = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "df_eliezer_test['Label'] = ['Eliezer' for s in range(df_eliezer_test.shape[0])]\n",
    "\n",
    "# PCA NO DATASET DE TREINO DAS OUTRAS PESSOAS (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "\n",
    "#principalComponents = pca.fit_transform(df_celso_train)\n",
    "#df_celso_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "#df_celso_train['Label'] = ['Celso' for s in range(df_celso_train.shape[0])]\n",
    "\n",
    "principalComponents = pca.fit_transform(df_thiago_train)\n",
    "df_thiago_train = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "df_thiago_train['Label'] = ['Thiago' for s in range(df_thiago_train.shape[0])]\n",
    "\n",
    "# CONCATENAR OS DOIS DATASETS DE TREINO (ALTERAR PARA CADA TESTE DIFERENTE)\n",
    "#df_train = pd.concat([df_evandro_train, df_celso_train]).sample(frac=1).reset_index(drop=True)\n",
    "#df_test = df_evandro_test\n",
    "\n",
    "df_train = pd.concat([df_eliezer_train, df_thiago_train]).sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_eliezer_test\n",
    "\n",
    "df_train.columns = 'PC1 PC2 PC3 Label'.split()\n",
    "df_test.columns = 'PC1 PC2 PC3 Label'.split()\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Label', axis=1)\n",
    "Y_train = df_train['Label']\n",
    "\n",
    "X_test = df_test.drop('Label', axis=1)\n",
    "Y_test = df_test['Label']\n",
    "\n",
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for the best k parameter\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1,50,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    Y_pred = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(Y_pred != Y_test))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,50,2), error_rate, color='blue', lw=1, ls='dashed', marker='o', markerfacecolor='red')\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=99)\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: {}%\".format(round(accuracy_score(Y_test, pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
